
# program		NN_BP_4L.py
# purpose	    make a NN learn from weather features to predict precipitation ammount
# usage         script
# notes         
#               (1) This version uses 1 extra layer than NN_BP_Eval.py it also uses more nodes. starts at 64 compared to the 32 BP_Eval had. Furthermore, New optimizers have been tested. With the extra layers autograd seems to show some convergence. 
#               (2) From scikitLearn, used train test split and many scikitlearn preprocessing tools.
#               (3) used matplotlib and made a list so that plotiing loss vs epochs was possible
#               (4) used a linear model and a MSE criterion b/c objective required non classifer models
#               (5) used a bayesian optimzer due to vast nature of hyperparameter tuning
#               (6) used dynamic patience based on the training loss, e.g., increase the patience if the loss
#               (7) used dynamic weights, intial weights are static, used xavier normal. intial weight is tailord such that features that make up precipitaion are heavier
#               (8) csv file is included, look at features that offered.
#               
# date			2/4/2023



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score
from torch.optim import Adam
import torch.nn.functional as F
from bayes_opt import BayesianOptimization
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Step 2: Load the data
df = pd.read_csv('cleaned_data_dallas_tx.csv')
# Step 3: Clean and preprocess the data
# Handle missing values
df.fillna(0, inplace=True)


#Define features and predictions
X = df.drop(['precip'], axis=1)
y = df['precip']

# Scale the data
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
# Normalize the data to zero mean and unit variance
df = (df - df.mean()) / df.std()

scaler = StandardScaler()
standardized_array = scaler.fit_transform(df.values)


# Step 4: Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

feature_weights = torch.Tensor([3, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1])

class Net(nn.Module):
    def __init__(self, feature_weights):
        super(Net, self).__init__()
        self.feature_weights = feature_weights
        self.fc1 = nn.Linear(X_train.shape[1], 64)
        nn.init.xavier_normal_(self.fc1.weight)
        self.fc2 = nn.Linear(64, 32)
        nn.init.xavier_normal_(self.fc1.weight)
        self.fc3 = nn.Linear(32, 16)
        nn.init.xavier_normal_(self.fc1.weight)
        self.fc4 = nn.Linear(16, 1)
        

    def forward(self, x):
        x = x * self.feature_weights.view(1, -1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

model = Net(feature_weights)

# Define the loss function and optimizer
criterion = nn.MSELoss()

# Step 7: Train the model with Bayesian optimization for optimizer's hyperparameters
test_losses = []
def train_nn(learning_rate, weight_decay):
    optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    best_loss = float("inf")
    wait = 0
    patience = 10
   
    
    for epoch in range(50):
        optimizer.zero_grad()
        outputs = model(torch.Tensor(X_train))
        loss = criterion(outputs, torch.Tensor(y_train.values).reshape(-1, 1))
        
        loss.backward()
        optimizer.step()
        with torch.no_grad():
            y_pred = model(torch.Tensor(X_test))
            test_loss = criterion(y_pred, torch.Tensor(y_test.values).reshape(-1, 1))
            test_losses.append(test_loss.item())
            

            if test_loss < best_loss:
                best_loss = test_loss
                wait = 0
                patience = max(patience, int(epoch * 0.1))
            else:
                wait += 1

            if wait >= patience:
                print(f'Early stopping after {epoch+1} epochs')
                break
    
    return test_loss.item()
bayesian_optimizer = BayesianOptimization(train_nn, {'learning_rate': (0.01, 0.1), 'weight_decay': (0.01, 0.1)})
bayesian_optimizer.maximize(init_points=100, n_iter=100)
print(bayesian_optimizer.max)

# Plot the training loss over time

plt.plot(range(len(test_losses)), test_losses)
# Label the axes
#plt.ylim(top=500)  # set the top limit of the y-axis to 100

plt.xlabel('Epoch')
plt.ylabel('Loss')

# Show the plot
plt.show()

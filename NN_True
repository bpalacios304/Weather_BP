


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score
from torch.optim import Adam
import torch.nn.functional as F
from bayes_opt import BayesianOptimization
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
# Step 2: Load the data
df = pd.read_csv('cleaned_data_dallas_tx.csv')
# Step 3: Clean and preprocess the data
# Handle missing values
df.fillna(0, inplace=True)

# Scale the data
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
# Normalize the data to zero mean and unit variance
df = (df - df.mean()) / df.std()

#scaler = StandardScaler()
#standardized_array = scaler.fit_transform(df.values)

#Define features and predictions
X = df.drop(['precip'], axis=1)
y = df['precip']


# Step 4: Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

feature_weights = torch.Tensor([3, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1])

class Net(nn.Module):
    def __init__(self, feature_weights):
        super(Net, self).__init__()
        self.feature_weights = feature_weights
        self.fc1 = nn.Linear(X_train.shape[1], 32)
        nn.init.xavier_normal_(self.fc1.weight)
        self.fc2 = nn.Linear(32, 16)
        nn.init.xavier_normal_(self.fc1.weight)
        self.fc3 = nn.Linear(16, 1)
        nn.init.xavier_normal_(self.fc1.weight)

    def forward(self, x):
        x = x * self.feature_weights.view(1, -1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = Net(feature_weights)

# Define the loss function and optimizer
criterion = nn.MSELoss()

# Step 7: Train the model with Bayesian optimization for optimizer's hyperparameters
test_losses = []
def train_nn(learning_rate, weight_decay):
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    best_loss = float("inf")
    wait = 0
    patience = 10
   
    
    for epoch in range(5000):
        optimizer.zero_grad()
        outputs = model(torch.Tensor(X_train))
        loss = criterion(outputs, torch.Tensor(y_train.values).reshape(-1, 1))
        
        loss.backward()
        optimizer.step()
        with torch.no_grad():
            y_pred = model(torch.Tensor(X_test))
            test_loss = criterion(y_pred, torch.Tensor(y_test.values).reshape(-1, 1))
            test_losses.append(test_loss.item())
            

            if test_loss < best_loss:
                best_loss = test_loss
                wait = 0
                patience = max(patience, int(epoch * 0.5))
            else:
                wait += 1

            if wait >= patience:
                print(f'Early stopping after {epoch+1} epochs')
                break
    
    return test_loss.item()
bayesian_optimizer = BayesianOptimization(train_nn, {'learning_rate': (0.01, 0.1), 'weight_decay': (0.01, 0.1)})
bayesian_optimizer.maximize(init_points=100, n_iter=50)
print(bayesian_optimizer.max)

# Plot the training loss over time

plt.plot(range(len(test_losses)), test_losses)
# Label the axes

plt.xlabel('Epoch')
plt.ylabel('Loss')

# Show the plot
plt.show()
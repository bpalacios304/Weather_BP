import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score
from torch.optim import Adam
import torch.nn.functional as F
from bayes_opt import BayesianOptimization
import matplotlib.pyplot as plt
# Step 2: Load the data
df = pd.read_csv('cleaned_data_dallas_tx.csv')
# Step 3: Clean and preprocess the data
# Handle missing values
df.fillna(0, inplace=True)


# Scale the data
scaler = MinMaxScaler()
X = df.drop(['precip'], axis=1)
y = df['precip']
X = scaler.fit_transform(X)

# Step 4: Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Step 5: Create the model
feature_weights = torch.Tensor([3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1])

class Net(nn.Module):
    def __init__(self, feature_weights):
        super(Net, self).__init__()
        self.feature_weights = feature_weights
        self.fc1 = nn.Linear(X_train.shape[1], 32)
        self.fc2 = nn.Linear(32, 16)
        self.fc3 = nn.Linear(16, 1)

    def forward(self, x):
        x = x * self.feature_weights.view(1, -1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = Net(feature_weights)

# Define the loss function and optimizer
criterion = nn.MSELoss()

# Step 7: Train the model with Bayesian optimization for optimizer's hyperparameters
def train_nn(learning_rate, weight_decay):
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    best_loss = float("inf")
    wait = 1
    patience = 10
   
    
    for epoch in range(1000):
        optimizer.zero_grad()
        outputs = model(torch.Tensor(X_train))
        loss = criterion(outputs, torch.Tensor(y_train.values).reshape(-1, 1))
        
        loss.backward()
        optimizer.step()
        with torch.no_grad():
            y_pred = model(torch.Tensor(X_test))
            test_loss = criterion(y_pred, torch.Tensor(y_test.values).reshape(-1, 1))
            
            
            if test_loss < best_loss:
                best_loss = test_loss
                wait = 0
            else:
                wait += 1

            if wait >= patience:
                print(f'Early stopping after {epoch+1} epochs')
                break
    plt.plot(test_loss)
    return test_loss.item()
bayesian_optimizer = BayesianOptimization(train_nn, {'learning_rate': (0.0001, 0.1), 'weight_decay': (0.0001, 0.1)})
bayesian_optimizer.maximize(init_points=15, n_iter=30)
print(bayesian_optimizer.max)
# Plot the training loss over time


# Label the axes
plt.xlabel('Epoch')
plt.ylabel('Loss')

# Show the plot
plt.show()